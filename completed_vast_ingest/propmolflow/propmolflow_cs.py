"""
author: Gregory Wolfe

Properties
----------

Other properties added to metadata
----------------------------------


File notes
----------
one extxyz file
Header example:
Properties=
    species:S:1:pos:R:3:forces:R:3
    GenAI=PropMolFlow
    target=alpha
    task=OOD
    target_value=100.0
    cv=27.95
    gap=0.23959
    lumo=-0.00223
    homo=-0.24182
    alpha=102.837
    mu=1.1052
    energy=-11504.860566391353
    pbc="F F F"

"""

import logging
import os
import sys
from pathlib import Path
from pickle import load
from time import time

import pyspark
from colabfit.tools.property_definitions import atomic_forces_pd, energy_pd
from colabfit.tools.vast.configuration_set import configuration_set_info
from colabfit.tools.vast.database import DataManager, VastDataLoader
from colabfit.tools.vast.property import PropertyMap, property_info
from dotenv import load_dotenv
from pyspark.sql import SparkSession

handler = logging.StreamHandler(sys.stdout)
handler.setLevel(logging.INFO)
formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
handler.setFormatter(formatter)
logger = logging.getLogger(f"{__name__}.hasher")
logger.setLevel("INFO")
logger.addHandler(handler)

logging.info(f"pyspark version: {pyspark.__version__}")
load_dotenv()
SLURM_JOB_ID = os.getenv("SLURM_JOB_ID")

n_cpus = os.getenv("SLURM_CPUS_PER_TASK")
if not n_cpus:
    n_cpus = 1

spark_ui_port = os.getenv("__SPARK_UI_PORT")
jars = os.getenv("VASTDB_CONNECTOR_JARS")
spark_session = (
    SparkSession.builder.appName(f"colabfit_{SLURM_JOB_ID}")
    .master(f"local[{n_cpus}]")
    .config("spark.executor.memoryOverhead", "600")
    .config("spark.ui.port", f"{spark_ui_port}")
    .config("spark.jars", jars)
    .config("spark.ui.showConsoleProgress", "true")
    .config("spark.driver.maxResultSize", 0)
    .config("spark.sql.adaptive.enabled", "true")
    .getOrCreate()
)


loader = VastDataLoader(
    spark_session=spark_session,
    access_key=os.getenv("SPARK_ID"),
    access_secret=os.getenv("SPARK_KEY"),
    endpoint=os.getenv("SPARK_ENDPOINT"),
)


# loader.metadata_dir = "test_md/MDtest"
# loader.config_table = "ndb.colabfit.dev.co_pmf"
# loader.prop_object_table = "ndb.colabfit.dev.po_pmf"
# loader.config_set_table = "ndb.colabfit.dev.cs_pmf"
# loader.dataset_table = "ndb.colabfit.dev.ds_pmf"
# loader.co_cs_map_table = "ndb.colabfit.dev.cs_co_map_pmf"


loader.config_table = "ndb.colabfit.dev.co_wip"
loader.prop_object_table = "ndb.colabfit.dev.po_wip"
loader.config_set_table = "ndb.colabfit.dev.cs_wip"
loader.dataset_table = "ndb.colabfit.dev.ds_wip"
loader.co_cs_map_table = "ndb.colabfit.dev.cs_co_map_wip"

logging.info(loader.config_table)
logging.info(loader.config_set_table)
logging.info(loader.dataset_table)
logging.info(loader.prop_object_table)

DATASET_FP = Path(
    "/scratch/gw2338/vast/data-lake-main/spark/scripts/gw_scripts/propmolflo"  # noqa
)
DATASET_NAME = "PropMolFlow_QM9_CNOFH_2025"
DATASET_ID = "DS_6qqf55wad1mv_0"
DESCRIPTION = " This DFT dataset is curated in response to the growing interest in property-guided molecule genaration using generative AI models. Typically, the properties of generated molecules are evaluated using machine learning (ML) property predictors trained on fully relaxed dataset. However, since generated molecules may deviate significantly from relaxed structures, these predictors can be highly unreliable for assessing their quality. This data provides DFT-evaluated properties, energy and forces for generated molecules. These structures are unrelaxed and can serve as a validation set for machine learning property predictors used in conditional molecule generation. It includes 9,387 molecules generated using PropMolFlow, a state-of-the-art conditional molecule generation model. PropMolFlow employs a flow matching process parameterized with an SE(3)-equivariant graph neural network. PropMolFlow models are trained on QM9 dataset. Molecules are generated by conditioning on six properties---polarizibility, gap, HOMO, LUMO, dipole moment and heat capacity at room temperature 298K---across two tasks: in-distribution and out-of-distribution generation. Full details are available in the corresponding paper."  # noqa
DOI = None
PUBLICATION_YEAR = "2025"
AUTHORS = [
    "Cheng Zeng",
    "Jirui Jin",
    "George Karypis",
    "Mark Transtrum",
    "Ellad B. Tadmor",
    "Richard G. Hennig",
    "Adrian Roitberg",
    "Stefano Martiniani",
    "Mingjie Liu",
]
LICENSE = "CC-BY-4.0"
PUBLICATION = "https://doi.org/10.1002/adma.202210788"
DATA_LINK = "https://alexandria.icams.rub.de/"


property_map = PropertyMap([atomic_forces_pd, energy_pd])
property_map.set_metadata_field("software", "Gaussian-16")
property_map.set_metadata_field("method", "B3LYP/6-31G(2df,p)")
property_map.set_metadata_field(
    "input", "Energy and force cutoffs are default used in Gaussian-16"
)

energy_info = property_info(
    property_name="energy",
    field="energy",
    units="eV",
    original_file_key="energy",
    additional=[("per-atom", {"value": False, "units": None})],
)
force_info = property_info(
    property_name="atomic-forces",
    field="forces",
    units="eV/angstrom",
    original_file_key="forces",
    additional=None,
)

property_map.set_properties([energy_info, force_info])

PROPERTY_MAP = property_map.get_property_map()

# "co_name_match",
# "co_label_match",
# "cs_name",
# "cs_description",
# "ordered",
CSS = [
    configuration_set_info(
        co_name_match="_ID_",
        cs_name="PropMolFlow_QM9_CNOFH_2025_ID",
        cs_description="In-domain configurations from the PropMolFlow QM9 CNOFH 2025 dataset",  # noqa
        co_label_match=None,
        ordered=False,
    ),
    configuration_set_info(
        co_name_match="_OOD_",
        cs_name="PropMolFlow_QM9_CNOFH_2025_OOD",
        cs_description="Out-of-domain configurations from the PropMolFlow QM9 CNOFH 2025 dataset",  # noqa
        co_label_match=None,
        ordered=False,
    ),
]


def read_pickle(fp: Path):
    with fp.open("rb") as f:
        for config in load(f):
            yield config


def read_wrapper(dir_path: str):
    dir_path = Path(dir_path)
    if not dir_path.exists():
        logging.info(f"Path {dir_path} does not exist")
        return
    all_json_paths = sorted(list(dir_path.rglob("*.pickle")), key=lambda x: x.stem)
    for path in all_json_paths:
        with open("configuration_set_map.csv", "a") as csf:
            logging.info(f"Processing file: {path}")
            reader = read_pickle(path)
            for config in reader:
                cs = config.info.get("task", None)
                csf.write(f"{cs},{config.id}\n")
                yield config


def main():
    t0 = time()
    config_generator = read_wrapper(DATASET_FP)
    logging.info("Creating DataManager")
    dm = DataManager(
        configs=config_generator,
        prop_defs=[energy_pd, atomic_forces_pd],
        prop_map=PROPERTY_MAP,
        dataset_id=DATASET_ID,
        standardize_energy=True,
        read_write_batch_size=10000,
    )
    logging.info(f"Time to prep: {time() - t0}")
    t1 = time()
    dm.create_configuration_sets(loader, CSS)
    t2 = time()
    logging.info(f"Time to load: {t2 - t1}")
    logging.info(f"Total time: {time() - t0}")
    logging.info("complete")


if __name__ == "__main__":
    main()
