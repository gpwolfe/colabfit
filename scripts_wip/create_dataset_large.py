"""
Script to update the OC20 dataset by batches, iterating across files of DO-ids
generated by oc20_is2res_batch_configs.py.
Supply a json file of dataset data, including the dataset id, name, authors, links,
description, license, and a path to a file of configuration set ids.

usage:

python oc20_is2res_batch_ds_update.py -i localhost -r 27017 -d oc20 -p 4 \
    -f /path/to/dataset_data.json


Notes
-----
The dataset data json file should have the following format:

{
    "dataset_id": "<colabfit-id>",
    "dataset_name": "ANI-2x-wB97MV-def2TZVPP",
    "authors": [
        "Kate Huddleston",
        "Roman Zubatyuk",
        "Justin Smith",
        "Adrian Roitberg",
        "Olexandr Isayev",
        "Ignacio Pickering",
        "Christian Devereux",
        "Kipton Barros"
    ],
    "publication": "https://doi.org/10.1021/acs.jctc.0c00121",
    "data_link": "https://doi.org/10.5281/zenodo.10108942",
    "description": "ANI-2x-wB97MV-def2TZVPP is ... which includes ...",
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "cs_ids_fp": "/path/to/cs_ids.txt"
}
"""

import itertools
import json
import logging
import sys
import time
from argparse import ArgumentParser
from functools import wraps
from pathlib import Path

import pymongo

from colabfit.tools.database import MongoDatabase

MAX_AUTO_RECONNECT_ATTEMPTS = 100


def auto_reconnect(mongo_func):
    """Gracefully handle a reconnection event."""

    @wraps(mongo_func)
    def wrapper(*args, **kwargs):
        for attempt in range(MAX_AUTO_RECONNECT_ATTEMPTS):
            try:
                return mongo_func(*args, **kwargs)
            except pymongo.errors.AutoReconnect as e:
                wait_t = 0.5 * pow(2, attempt)  # exponential back off
                if wait_t > 1800:
                    wait_t = 1800  # cap at 1/2 hour
                logging.warning(
                    "PyMongo auto-reconnecting... %s. Waiting %.1f seconds.",
                    str(e),
                    wait_t,
                )
                time.sleep(wait_t)

    return wrapper


def do_file_handler(do_id_fp):
    with open(do_id_fp, "r") as f:
        do_ids = [id.strip() for id in f.readlines()]
    return do_ids


@auto_reconnect
# async
def main(argv):
    parser = ArgumentParser()
    parser.add_argument("-i", "--ip", type=str, help="IP of host mongod")
    parser.add_argument(
        "-d",
        "--db_name",
        type=str,
        help="Name of MongoDB database to add dataset to",
        required=True,
    )
    parser.add_argument(
        "-p",
        "--nprocs",
        type=int,
        help="Number of processors to use for job",
        default=4,
    )
    parser.add_argument("-r", "--port", type=int, help="Target port for MongoDB")
    parser.add_argument("-f", "--ds_data", type=Path, help="File of dataset data")
    args = parser.parse_args(argv)
    with open(args.ds_data, "r") as f:
        ds_data = json.load(f)

    do_ids_dir = Path(ds_data["do_ids_dir"])

    ds_id = ds_data["dataset_id"]
    ds_name = ds_data["dataset_name"]
    authors = ds_data["authors"]
    publication = ds_data["publication"]
    publication_year = ds_data["publication_year"]
    data_link = ds_data["data_link"]
    ds_desc = ds_data["description"]
    ds_license = ds_data["license"]
    cs_ids_fp = ds_data["cs_ids_fp"]
    if cs_ids_fp is not None:
        with open(cs_ids_fp, "r") as f:
            cs_ids = [id.strip() for id in f.readlines()]
    print(f"DS-ID: {ds_id}", flush=True)
    nprocs = args.nprocs
    client = MongoDatabase(
        args.db_name, nprocs=nprocs, uri=f"mongodb://{args.ip}:{args.port}"
    )
    fps = sorted(list(do_ids_dir.rglob("*.txt")))
    print("nfiles", len(fps), flush=True)
    do_hashes = list(
        itertools.chain.from_iterable(map(do_file_handler, [fp for fp in fps]))
    )
    if cs_ids_fp is not None:
        client.insert_dataset(
            do_hashes=do_hashes,
            ds_id=ds_id,
            name=ds_name,
            authors=authors,
            cs_ids=cs_ids,
            publication_link=publication,
            data_link=data_link,
            publication_year=publication_year,
            description=ds_desc,
            verbose=True,
            data_license=ds_license,
        )
    else:
        client.insert_dataset(
            do_hashes=do_hashes,
            ds_id=ds_id,
            name=ds_name,
            authors=authors,
            publication_link=publication,
            data_link=data_link,
            publication_year=publication_year,
            description=ds_desc,
            verbose=True,
            data_license=ds_license,
        )
    client.close()


if __name__ == "__main__":
    args = sys.argv[1:]
    main(args)
