"""
Script to update the OC20 dataset by batches, iterating across files of DO-ids
generated by oc20_is2res_batch_configs.py.

usage:

python oc20_is2res_batch_ds_update.py -i localhost -r 27017 -d oc20 -p 4 \
    -o DS_hcrq1don4k1r_0_do_ids_2024_01_05


Notes
-----
Configuration set ids have been added manually as a list. These are referenced
only once, in the case that the dataset has not yet been created. This is the only time
they are required for the dataset object.

"""

from argparse import ArgumentParser

# import asyncio
from collections import defaultdict
import datetime
from functools import partial, wraps
import itertools
import logging
from pathlib import Path
import pymongo
from multiprocessing import Pool
import sys
import time
import numpy as np
from colabfit.tools.database import MongoDatabase
from colabfit import (
    SHORT_ID_STRING_NAME,
)
from tqdm import tqdm

MAX_AUTO_RECONNECT_ATTEMPTS = 100


# DATASET_FP = Path("data/ani2x/final_h5")
DATASET_NAME = "ANI-2x-wB97MV-def2TZVPP"

LICENSE = "https://creativecommons.org/licenses/by/4.0"

PUBLICATION = "https://doi.org/10.1021/acs.jctc.0c00121"
DATA_LINK = "https://doi.org/10.5281/zenodo.10108942"
# OTHER_LINKS = []

AUTHORS = [
    "Kate Huddleston",
    "Roman Zubatyuk",
    "Justin Smith",
    "Adrian Roitberg",
    "Olexandr Isayev",
    "Ignacio Pickering",
    "Christian Devereux",
    "Kipton Barros",
]
DATASET_DESC = (
    "ANI-2x-wB97MV-def2TZVPP is a portion of the ANI-2x dataset, which includes "
    "DFT-calculated energies for structures from 2 to 63 atoms in size containing "
    "H, C, N, O, S, F, and Cl. This portion of ANI-2x was calculated at the WB97MV "
    "level of theory using the def2TZVPP basis set. Configuration sets are divided "
    "by number of atoms per structure."
)

cs_ids = [
    "CS_knqfuvyz6hvd_0",
    "CS_388n2md9es5w_0",
    "CS_y8wx1zd5osz5_0",
    "CS_6ln67044rix3_0",
    "CS_9jwjgajjyvhd_0",
    "CS_96b0y2vfsovc_0",
    "CS_9fekwm65w63y_0",
    "CS_k9wh7ydtiq28_0",
    "CS_0dk2ktrqgju8_0",
    "CS_b3enivbtp9mp_0",
    "CS_2xa45zo5fwak_0",
    "CS_6os541btg0yk_0",
    "CS_h390sodyz8jf_0",
    "CS_pzq68semsnu0_0",
    "CS_e0nki6w5ykam_0",
    "CS_ccyz1hgzkkrl_0",
    "CS_sztquf6aqjwb_0",
    "CS_7qc7nllbnt4j_0",
    "CS_clveqfttm1m0_0",
    "CS_hxd9sd7d37ub_0",
    "CS_gignoq9d8nvt_0",
    "CS_pigtmthn19t1_0",
    "CS_nrs2ye0evma5_0",
    "CS_ncbmbzrica68_0",
    "CS_1bq1mqeujw97_0",
    "CS_qpbkt9hn7nrd_0",
    "CS_d2y8q05puxb3_0",
    "CS_sz0bsjj1es5s_0",
    "CS_3olf902d7j4j_0",
    "CS_lrs24i3vt46m_0",
    "CS_ys83rfknk9hc_0",
    "CS_ebsdkrtsgkgl_0",
    "CS_oofiqv0utf27_0",
    "CS_u8253sojyo4a_0",
    "CS_5h3pa40v4s43_0",
    "CS_uze6o25xs6fj_0",
    "CS_78doqhpf1eq6_0",
    "CS_huoqormjuj69_0",
    "CS_vvky9rkobo66_0",
    "CS_4tvanb3102hk_0",
    "CS_c524m9l2wmg5_0",
    "CS_uzmcamxbha4m_0",
    "CS_dksoscwcjvka_0",
    "CS_86lww7evy75v_0",
    "CS_0h30zpvxkqvx_0",
    "CS_5zfo9iaw9gyf_0",
    "CS_3u23qscjs5op_0",
    "CS_rg4vefuxo062_0",
    "CS_tzfupzllwtbi_0",
    "CS_opnxoox9zmvy_0",
    "CS_gb5ltvksqwtt_0",
    "CS_dw0gj0t9ejdp_0",
    "CS_3g4ufbqmwy8r_0",
    "CS_0x97c60e485l_0",
    "CS_128aefaux4db_0",
    "CS_8z9hd8i75i9t_0",
    "CS_iecelo453ox7_0",
    "CS_lal8jez1tu3p_0",
    "CS_iz2x7oktmzxi_0",
]


def auto_reconnect(mongo_func):
    """Gracefully handle a reconnection event."""

    @wraps(mongo_func)
    def wrapper(*args, **kwargs):
        for attempt in range(MAX_AUTO_RECONNECT_ATTEMPTS):
            try:
                return mongo_func(*args, **kwargs)
            except pymongo.errors.AutoReconnect as e:
                wait_t = 0.5 * pow(2, attempt)  # exponential back off
                if wait_t > 1800:
                    wait_t = 1800  # cap at 1/2 hour
                logging.warning(
                    "PyMongo auto-reconnecting... %s. Waiting %.1f seconds.",
                    str(e),
                    wait_t,
                )
                time.sleep(wait_t)

    return wrapper


# function dependency for aggregate configuration summary
@auto_reconnect
def agg(hashes, db, uri):
    from colabfit.tools.database import MongoDatabase

    client = MongoDatabase(db, uri=uri)
    proxy = {
        "nsites": 0,
        "chemical_systems": set(),
        "elements": [],
        "individual_elements_ratios": {},
        "total_elements_ratios": {},
        "chemical_formula_reduced": set(),
        "chemical_formula_anonymous": set(),
        "chemical_formula_hill": set(),
        "nperiodic_dimensions": set(),
        "dimension_types": set(),
    }

    docs = client.configurations.find(
        {"hash": {"$in": hashes}},
        {
            "nsites": 1,
            "elements": 1,
            "elements_ratios": 1,
            "total_elements_ratios": 1,
            "nperiodic_dimensions": 1,
            "dimension_types": 1,
            "elements_ratios": 1,
            "chemical_formula_anonymous": 1,
            "chemical_formula_hill": 1,
            "chemical_formula_reduced": 1,
        },
    )
    while True:
        for doc in docs:
            proxy["nsites"] += doc["nsites"]
            proxy["chemical_systems"].add("".join(doc["elements"]))

            for e, er in zip(doc["elements"], doc["elements_ratios"]):
                if e not in proxy["elements"]:
                    proxy["elements"].append(e)
                    proxy["total_elements_ratios"][e] = er * doc["nsites"]
                    proxy["individual_elements_ratios"][e] = {np.round(er, decimals=2)}
                else:
                    proxy["total_elements_ratios"][e] += er * doc["nsites"]
                    proxy["individual_elements_ratios"][e].add(np.round(er, decimals=2))

            proxy["chemical_formula_reduced"].add(doc["chemical_formula_reduced"])
            proxy["chemical_formula_anonymous"].add(doc["chemical_formula_anonymous"])
            proxy["chemical_formula_hill"].add(doc["chemical_formula_hill"])

            proxy["nperiodic_dimensions"].add(doc["nperiodic_dimensions"])
            proxy["dimension_types"].add(tuple(doc["dimension_types"]))
        return proxy


@auto_reconnect
def aggregate_configuration_summaries(client, hashes, verbose=False):
    """
    Uses agg to gather batch of config aggregated info
    """
    aggregated_info = {
        "nconfigurations": len(hashes),
        "nsites": 0,
        "nelements": 0,
        "chemical_systems": set(),
        "elements": [],
        "individual_elements_ratios": {},
        "total_elements_ratios": {},
        "chemical_formula_reduced": set(),
        "chemical_formula_anonymous": set(),
        "chemical_formula_hill": set(),
        "nperiodic_dimensions": set(),
        "dimension_types": set(),
    }

    n = len(hashes)
    k = client.nprocs
    chunked_hashes = [
        hashes[i * (n // k) + min(i, n % k) : (i + 1) * (n // k) + min(i + 1, n % k)]
        for i in range(k)
    ]
    s = time.time()
    with Pool(k) as pool:
        aggs = pool.map(
            partial(agg, db=client.database_name, uri=client.uri), chunked_hashes
        )
    for a in aggs:
        aggregated_info["nsites"] += a["nsites"]
        aggregated_info["chemical_systems"].update(a["chemical_systems"])

        for e, er in zip(a["elements"], a["individual_elements_ratios"]):
            if e not in aggregated_info["elements"]:
                aggregated_info["elements"].append(e)
                aggregated_info["total_elements_ratios"][e] = a[
                    "total_elements_ratios"
                ][e]
                aggregated_info["individual_elements_ratios"][e] = a[
                    "individual_elements_ratios"
                ][e]
            else:
                aggregated_info["total_elements_ratios"][e] += a[
                    "total_elements_ratios"
                ][e]
                aggregated_info["individual_elements_ratios"][e].update(
                    a["individual_elements_ratios"][e]
                )

        aggregated_info["chemical_formula_reduced"].update(
            a["chemical_formula_reduced"]
        )
        aggregated_info["chemical_formula_anonymous"].update(
            a["chemical_formula_anonymous"]
        )
        aggregated_info["chemical_formula_hill"].update(a["chemical_formula_hill"])

        aggregated_info["nperiodic_dimensions"].update(a["nperiodic_dimensions"])
        aggregated_info["dimension_types"].update(a["dimension_types"])

    for e in aggregated_info["elements"]:
        aggregated_info["nelements"] += 1
        aggregated_info["total_elements_ratios"][e] /= aggregated_info["nsites"]
        aggregated_info["individual_elements_ratios"][e] = list(
            aggregated_info["individual_elements_ratios"][e]
        )

    aggregated_info["chemical_systems"] = list(aggregated_info["chemical_systems"])
    aggregated_info["chemical_formula_reduced"] = list(
        aggregated_info["chemical_formula_reduced"]
    )
    aggregated_info["chemical_formula_anonymous"] = list(
        aggregated_info["chemical_formula_anonymous"]
    )
    aggregated_info["chemical_formula_hill"] = list(
        aggregated_info["chemical_formula_hill"]
    )
    aggregated_info["nperiodic_dimensions"] = list(
        aggregated_info["nperiodic_dimensions"]
    )
    aggregated_info["dimension_types"] = list(aggregated_info["dimension_types"])
    print("Configuration aggregation time:", time.time() - s, flush=True)
    return aggregated_info


@auto_reconnect
def aggregate_data_object_info(client, do_hashes, verbose=False):
    """
    Query data objects by hash, getting associated configurations and
    returning the aggregated configuration data

    Uses aggregate_configuration_summaries to gather info from configurations
    pointing to current batch of DOs
    """
    if isinstance(do_hashes, str):
        do_hashes = [do_hashes]

    # property_types = defaultdict(int)
    # aggregated_info = {}
    # co_ids = []
    pipeline = [
        {"$match": {"hash": {"$in": do_hashes}}},
        {
            "$facet": {
                "typesCount": [
                    {"$unwind": "$property_types"},
                    {"$group": {"_id": "$property_types", "count": {"$sum": 1}}},
                ],
                "configuration_ids": [
                    {
                        "$group": {
                            "_id": None,
                            "configuration_ids": {
                                "$addToSet": "$relationships.configuration"
                            },
                        }
                    },
                    {"$project": {"_id": 0, "configuration_ids": 1}},
                ],
            }
        },
    ]

    results = client.data_objects.aggregate(pipeline)
    results = next(results)
    p_types = [x["_id"] for x in results["typesCount"]]
    p_counts = [x["count"] for x in results["typesCount"]]
    co_ids = itertools.chain.from_iterable(
        results["configuration_ids"][0]["configuration_ids"]
    )
    co_ids = [x.replace("CO_", "") for x in co_ids]
    # for doc in tqdm(
    #     client.data_objects.find(
    #         {"hash": {"$in": do_hashes}},
    #         {"property_types": 1, "relationships.configuration": 1},
    #         batch_size=50,
    #     )
    # ):
    #     for p_type in doc["property_types"]:
    #         property_types[p_type] += 1
    #     # get co hashes
    #     co_ids.append(doc["relationships"][0]["configuration"].replace("CO_", ""))
    # config_agg = aggregate_configuration_summaries(client, co_ids, verbose=verbose)

    aggregated_info = aggregate_configuration_summaries(client, co_ids, verbose=verbose)
    aggregated_info["property_types"] = p_types
    aggregated_info["property_types_counts"] = p_counts
    # aggregated_info["property_types"] = list(property_types.keys())
    # aggregated_info["property_types_counts"] = list(property_types.values())
    aggregated_info["nconfigurations"] = len(co_ids)
    return aggregated_info


def update_ds_agg_info(aggregated_info, new_info):
    """
    Updates a dict of aggregated info

    aggregated_info = current ds agg info
    new_info = new info to add to current agg info
    """
    proxy = {
        "nconfigurations": aggregated_info["nconfigurations"],
        "nsites": aggregated_info["nsites"],
        "elements": aggregated_info["elements"],
        "total_elements_ratios": aggregated_info["total_elements_ratios"],
        "nperiodic_dimensions": set(aggregated_info["nperiodic_dimensions"]),
        "dimension_types": set([tuple(x) for x in aggregated_info["dimension_types"]]),
    }
    property_types = dict(
        zip(
            aggregated_info["property_types"],
            aggregated_info["property_types_counts"],
        )
    )
    new_num_e = defaultdict(int)
    proxy["nsites"] += new_info["nsites"]
    proxy["nconfigurations"] += new_info["nconfigurations"]

    for e in new_info["elements"]:
        new_num_e[e] += 1
        if e not in proxy["elements"]:
            proxy["elements"].append(e)

    proxy["nperiodic_dimensions"].update(new_info["nperiodic_dimensions"])
    proxy["dimension_types"].update(new_info["dimension_types"])
    for p_type, p_type_count in zip(
        new_info["property_types"], new_info["property_types_counts"]
    ):
        if p_type in property_types:
            property_types[p_type] += p_type_count
        else:
            property_types[p_type] = p_type_count

    # Reset elements and calculate new total elements ratios
    proxy["nelements"] = 0
    old_nsites = aggregated_info["nsites"]
    proxy["elements"] = sorted(proxy["elements"])
    for e in proxy["elements"]:
        old_ratio = aggregated_info["total_elements_ratios"].get(e, 0)
        old_num_e = old_ratio * old_nsites
        proxy["nelements"] += 1
        proxy["total_elements_ratios"][e] = (old_num_e + new_num_e[e]) / proxy["nsites"]

    proxy["nperiodic_dimensions"] = list(proxy["nperiodic_dimensions"])
    proxy["dimension_types"] = list(aggregated_info["dimension_types"])
    property_types = dict(sorted(property_types.items()))
    proxy["property_types"] = list(property_types.keys())
    proxy["property_types_counts"] = list(property_types.values())
    return proxy


@auto_reconnect
# async
def insert_dos_to_dataset(args, ds_id=None, verbose=False, fp=None):
    """
    Uses aggregate_data_object_info to gather info from a new batch of DOs

    Uses update_ds_agg_info to update an existing dataset's aggregated info
    and push to the new data to the database. Does not change dataset version.
    """
    client = MongoDatabase(
        args.db_name, nprocs=args.nprocs, uri=f"mongodb://{args.ip}:{args.port}"
    )
    print(client.name, "\n", flush=True)
    print(fp, "\n", flush=True)
    batch_size = 100000
    with open(fp, "r") as f:
        do_hashes = list(set([id.strip() for id in f.readlines()]))
    # new_aggregated_info = {}
    chunked_hashes = [
        do_hashes[i : i + batch_size] for i in range(0, len(do_hashes), batch_size)
    ]
    for hashes in chunked_hashes:
        if isinstance(hashes, str):
            hashes = [hashes]
        new_aggregated_info = aggregate_data_object_info(
            client, hashes, verbose=verbose
        )
        # for k, v in aggregate_data_object_info(
        #     client, do_hashes, verbose=verbose
        # ).items():
        #     new_aggregated_info[k] = v
        # client.insert_aggregated_info(aggregated_info, "dataset", ds_id)
        # Remove possible duplicates
        # do_hashes = list(set(do_hashes))
        # Insert necessary aggregated info into its collection
        client.insert_aggregated_info(new_aggregated_info, "dataset", ds_id)

        current_ds_agg_info = client.datasets.find(
            {SHORT_ID_STRING_NAME: ds_id},
            {
                f"aggregated_info.{key}": 1
                for key in [
                    "nconfigurations",
                    "nsites",
                    "nelements",
                    "elements",
                    "total_elements_ratios",
                    "nperiodic_dimensions",
                    "dimension_types",
                    "property_types",
                    "property_types_counts",
                ]
            },
        )
        current_ds_agg_info = next(current_ds_agg_info)
        current_ds_agg_info = current_ds_agg_info["aggregated_info"]

        # Now update the dataset with the latest iteration of aggregated info
        for item in [
            "individual_elements_ratios",
            "chemical_systems",
            "chemical_formula_anonymous",
            "chemical_formula_hill",
            "chemical_formula_reduced",
        ]:
            new_aggregated_info.pop(item)
        updated_aggregated_info = update_ds_agg_info(
            current_ds_agg_info, new_aggregated_info
        )

        client.datasets.update_one(
            {"colabfit-id": ds_id},
            {
                "$set": {
                    "aggregated_info": updated_aggregated_info,
                    "last_modified": datetime.datetime.now().strftime(
                        "%Y-%m-%dT%H:%M:%SZ"
                    ),
                },
            },
            upsert=True,
            hint="hash",
        )
        with open("ds_batches.txt", "a") as f:
            f.write(f"{fp}\n")
    client.close()


@auto_reconnect
# async
def main(argv):
    parser = ArgumentParser()
    parser.add_argument("-i", "--ip", type=str, help="IP of host mongod")
    parser.add_argument(
        "-d",
        "--db_name",
        type=str,
        help="Name of MongoDB database to add dataset to",
        default="----",
    )
    parser.add_argument(
        "-p",
        "--nprocs",
        type=int,
        help="Number of processors to use for job",
        default=4,
    )
    parser.add_argument("-r", "--port", type=int, help="Target port for MongoDB")
    parser.add_argument(
        "-o", "--do", type=Path, help="Directory of data_object id files"
    )
    parser.add_argument(
        "--ds", type=str, help="Dataset id to update with new data objects"
    )
    args = parser.parse_args(argv)
    do_ids_dir = Path(args.do)
    ds_id = args.ds
    print("DS-ID", ds_id, flush=True)
    nprocs = args.nprocs
    client = MongoDatabase(
        args.db_name, nprocs=nprocs, uri=f"mongodb://{args.ip}:{args.port}"
    )
    fps = sorted(list(do_ids_dir.rglob("*.txt")))
    print("nfiles", len(fps), flush=True)

    # check if dataset exists
    if client.datasets.find_one({"colabfit-id": ds_id}) is not None:
        for fp in fps:
            insert_dos_to_dataset(args, ds_id=ds_id, fp=fp)

        # tasks = [
        #     asyncio.create_task(insert_dos_to_dataset(args, ds_id=ds_id, fp=fp))
        #     for fp in fps
        # ]
        # await asyncio.gather(*tasks)

    else:
        # Create the dataset with the first file
        print("Creating dataset...", flush=True)
        with open(fps[0], "r") as f:
            do_ids0 = [id.strip() for id in f.readlines()]
        client.insert_dataset(
            do_hashes=do_ids0,
            ds_id=ds_id,
            name=DATASET_NAME,
            authors=AUTHORS,
            cs_ids=cs_ids,
            links=[PUBLICATION, DATA_LINK],  # + OTHER_LINKS,
            description=DATASET_DESC,
            verbose=False,
            data_license="https://creativecommons.org/licenses/by/4.0/",
        )
        client.close()
        print("Dataset created with first batch.", flush=True)
        # Create async tasks for update dataset
        for fp in fps[1:]:
            insert_dos_to_dataset(args, ds_id=ds_id, fp=fp)
        # tasks = [
        #     asyncio.create_task(insert_dos_to_dataset(args, ds_id=ds_id, fp=fp))
        #     for fp in fps[1:]
        # ]
        # await asyncio.gather(*tasks)


if __name__ == "__main__":
    args = sys.argv[1:]
    main(args)
    # asyncio.run(main(args))
