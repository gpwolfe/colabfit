"""
Script to update the OC20 dataset by batches, iterating across files of DO-ids
generated by oc20_is2res_batch_configs.py.

usage:

python oc20_is2res_batch_ds_update.py -i localhost -r 27017 -d oc20 -p 4 \
    -o DS_hcrq1don4k1r_0_do_ids_2024_01_05


Notes
-----
Configuration set ids have been added manually as a list. These are referenced
only once, in the case that the dataset has not yet been created. This is the only time
they are required for the dataset object.

"""

from argparse import ArgumentParser

# import asyncio
# from collections import defaultdict
# import datetime
# from functools import partial,
from functools import wraps
import itertools
import logging
from pathlib import Path
import pymongo

# from multiprocessing import Pool
# import re
import sys
import time

# import numpy as np
from colabfit.tools.database import MongoDatabase

# from colabfit import (
#     SHORT_ID_STRING_NAME,
# )

# from tqdm import tqdm

MAX_AUTO_RECONNECT_ATTEMPTS = 100


# DATASET_FP = Path("data/ani2x/final_h5")
DATASET_NAME = "ANI-2x-wB97MV-def2TZVPP"

LICENSE = "https://creativecommons.org/licenses/by/4.0"

PUBLICATION = "https://doi.org/10.1021/acs.jctc.0c00121"
DATA_LINK = "https://doi.org/10.5281/zenodo.10108942"

AUTHORS = [
    "Kate Huddleston",
    "Roman Zubatyuk",
    "Justin Smith",
    "Adrian Roitberg",
    "Olexandr Isayev",
    "Ignacio Pickering",
    "Christian Devereux",
    "Kipton Barros",
]
DATASET_DESC = (
    "ANI-2x-wB97MV-def2TZVPP is a portion of the ANI-2x dataset, which includes "
    "DFT-calculated energies for structures from 2 to 63 atoms in size containing "
    "H, C, N, O, S, F, and Cl. This portion of ANI-2x was calculated at the WB97MV "
    "level of theory using the def2TZVPP basis set. Configuration sets are divided "
    "by number of atoms per structure."
)


def auto_reconnect(mongo_func):
    """Gracefully handle a reconnection event."""

    @wraps(mongo_func)
    def wrapper(*args, **kwargs):
        for attempt in range(MAX_AUTO_RECONNECT_ATTEMPTS):
            try:
                return mongo_func(*args, **kwargs)
            except pymongo.errors.AutoReconnect as e:
                wait_t = 0.5 * pow(2, attempt)  # exponential back off
                if wait_t > 1800:
                    wait_t = 1800  # cap at 1/2 hour
                logging.warning(
                    "PyMongo auto-reconnecting... %s. Waiting %.1f seconds.",
                    str(e),
                    wait_t,
                )
                time.sleep(wait_t)

    return wrapper


def do_file_handler(do_id_fp):
    with open(do_id_fp, "r") as f:
        do_ids = [id.strip() for id in f.readlines()]
    return do_ids


@auto_reconnect
# async
def main(argv):
    parser = ArgumentParser()
    parser.add_argument("-i", "--ip", type=str, help="IP of host mongod")
    parser.add_argument(
        "-d",
        "--db_name",
        type=str,
        help="Name of MongoDB database to add dataset to",
        default="----",
    )
    parser.add_argument(
        "-p",
        "--nprocs",
        type=int,
        help="Number of processors to use for job",
        default=4,
    )
    parser.add_argument("-r", "--port", type=int, help="Target port for MongoDB")
    parser.add_argument(
        "-o", "--do", type=Path, help="Directory of data_object id files"
    )
    parser.add_argument(
        "--ds", type=str, help="Dataset id to update with new data objects"
    )
    parser.add_argument(
        "--cs",
        type=Path,
        help="File of configuration set ids to add to dataset",
        default=None,
    )
    args = parser.parse_args(argv)
    do_ids_dir = Path(args.do)
    ds_id = args.ds
    cs_ids_fp = args.cs
    with open(cs_ids_fp, "r") as f:
        cs_ids = [id.strip() for id in f.readlines()]
    print(f"DS-ID: {ds_id}", flush=True)
    nprocs = args.nprocs
    client = MongoDatabase(
        args.db_name, nprocs=nprocs, uri=f"mongodb://{args.ip}:{args.port}"
    )
    fps = sorted(list(do_ids_dir.rglob("*.txt")))
    print("nfiles", len(fps), flush=True)
    do_hashes = list(
        itertools.chain.from_iterable(map(do_file_handler, [fp for fp in fps]))
    )

    client.insert_dataset(
        do_hashes=do_hashes,
        ds_id=ds_id,
        name=DATASET_NAME,
        authors=AUTHORS,
        cs_ids=cs_ids,
        links=[PUBLICATION, DATA_LINK],  # + OTHER_LINKS,
        description=DATASET_DESC,
        verbose=True,
        data_license="https://creativecommons.org/licenses/by/4.0/",
    )
    client.close()


if __name__ == "__main__":
    args = sys.argv[1:]
    main(args)
    # asyncio.run(main(args))
