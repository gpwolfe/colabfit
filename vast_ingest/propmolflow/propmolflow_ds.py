"""
author: Gregory Wolfe

Properties
----------

Other properties added to metadata
----------------------------------


File notes
----------
one extxyz file
Header example:
Properties=
    species:S:1:pos:R:3:forces:R:3
    GenAI=PropMolFlow
    target=alpha
    task=OOD
    target_value=100.0
    cv=27.95
    gap=0.23959
    lumo=-0.00223
    homo=-0.24182
    alpha=102.837
    mu=1.1052
    energy=-11504.860566391353
    pbc="F F F"

"""

import os
from pathlib import Path
from ase.io import read
from time import time
import logging
import pyspark
from colabfit.tools.vast.configuration import AtomicConfiguration
from colabfit.tools.vast.database import DataManager, VastDataLoader
from colabfit.tools.vast.property import PropertyMap, property_info
from colabfit.tools.property_definitions import (
    atomic_forces_pd,
    energy_pd,
)
import sys
from dotenv import load_dotenv
from pyspark.sql import SparkSession

handler = logging.StreamHandler(sys.stdout)
handler.setLevel(logging.INFO)
formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
handler.setFormatter(formatter)
logger = logging.getLogger(f"{__name__}.hasher")
logger.setLevel("INFO")
logger.addHandler(handler)

logging.info(f"pyspark version: {pyspark.__version__}")
load_dotenv()
SLURM_JOB_ID = os.getenv("SLURM_JOB_ID")

n_cpus = os.getenv("SLURM_CPUS_PER_TASK")
if not n_cpus:
    n_cpus = 1

spark_ui_port = os.getenv("__SPARK_UI_PORT")
jars = os.getenv("VASTDB_CONNECTOR_JARS")
spark_session = (
    SparkSession.builder.appName(f"colabfit_{SLURM_JOB_ID}")
    .master(f"local[{n_cpus}]")
    .config("spark.executor.memoryOverhead", "600")
    .config("spark.ui.port", f"{spark_ui_port}")
    .config("spark.jars", jars)
    .config("spark.ui.showConsoleProgress", "true")
    .config("spark.driver.maxResultSize", 0)
    .config("spark.sql.adaptive.enabled", "true")
    .getOrCreate()
)


loader = VastDataLoader(
    spark_session=spark_session,
    access_key=os.getenv("SPARK_ID"),
    access_secret=os.getenv("SPARK_KEY"),
    endpoint=os.getenv("SPARK_ENDPOINT"),
)


# loader.metadata_dir = "test_md/MDtest"
# loader.config_table = "ndb.colabfit.dev.co_pmf"
# loader.prop_object_table = "ndb.colabfit.dev.po_pmf"
# loader.config_set_table = "ndb.colabfit.dev.cs_pmf"
# loader.dataset_table = "ndb.colabfit.dev.ds_pmf"
# loader.co_cs_map_table = "ndb.colabfit.dev.cs_co_map_pmf"


loader.config_table = "ndb.colabfit.dev.co_wip"
loader.prop_object_table = "ndb.colabfit.dev.po_wip"
loader.config_set_table = "ndb.colabfit.dev.cs_wip"
loader.dataset_table = "ndb.colabfit.dev.ds_wip"
loader.co_cs_map_table = "ndb.colabfit.dev.cs_co_map_wip"

logging.info(loader.config_table)
logging.info(loader.config_set_table)
logging.info(loader.dataset_table)
logging.info(loader.prop_object_table)

DATASET_FP = Path(
    "/scratch/gw2338/vast/data-lake-main/spark/scripts/gw_scripts/propmolflow/propmolflow.extxyz"  # noqa
)
DATASET_NAME = "PropMolFlow_QM9_CNOFH_2025"
DATASET_ID = "DS_6qqf55wad1mv_0"
DESCRIPTION = "This DFT dataset is curated in response to the growing interest in property-guided molecule genaration using generative AI models. Typically, the properties of generated molecules are evaluated using machine learning (ML) property predictors trained on fully relaxed dataset. However, since generated molecules may deviate significantly from relaxed structures, these predictors can be highly unreliable for assessing their quality. This data provides DFT-evaluated properties, energy and forces for generated molecules. These structures are unrelaxed and can serve as a validation set for machine learning property predictors used in conditional molecule generation. It includes 10,773 molecules generated using PropMolFlow, a state-of-the-art conditional molecule generation model. PropMolFlow employs a flow matching process parameterized with an SE(3)-equivariant graph neural network. PropMolFlow models are trained on QM9 dataset. Molecules are generated by conditioning on six properties---polarizibility, gap, HOMO, LUMO, dipole moment and heat capacity at room temperature 298K---across two tasks: in-distribution and out-of-distribution generation. Full details are available in the corresponding paper."  # noqa
DOI = None
PUBLICATION_YEAR = "2025"
AUTHORS = [
    "Cheng Zeng",
    "Jirui Jin",
    "George Karypis",
    "Mark Transtrum",
    "Ellad B. Tadmor",
    "Richard G. Hennig",
    "Adrian Roitberg",
    "Stefano Martiniani",
    "Mingjie Liu",
]
LICENSE = "CC-BY-4.0"
PUBLICATION = "https://arxiv.org/abs/2505.21469"
DATA_LINK = None


property_map = PropertyMap([atomic_forces_pd, energy_pd])
property_map.set_metadata_field("software", "Gaussian-16")
property_map.set_metadata_field("method", "B3LYP/6-31G(2df,p)")
property_map.set_metadata_field(
    "input", "Energy and force cutoffs are default used in Gaussian-16"
)

for field in [
    "target",
    "task",
    "target_value",
    "cv",
    "gap",
    "lumo",
    "homo",
    "alpha",
    "mu",
]:
    property_map.set_metadata_field(field, field, dynamic=True)


energy_info = property_info(
    property_name="energy",
    field="energy",
    units="eV",
    original_file_key="energy",
    additional=[("per-atom", {"value": False, "units": None})],
)
force_info = property_info(
    property_name="atomic-forces",
    field="forces",
    units="eV/angstrom",
    original_file_key="forces",
    additional=None,
)

property_map.set_properties([energy_info, force_info])

PROPERTY_MAP = property_map.get_property_map()


def read_wrapper(path: str):
    with open("configuration_set_map.csv", "a") as csf:
        logging.info(f"Processing file: {path}")
        reader = read(path, index=":", format="extxyz")
        for i, config in enumerate(reader):
            cs = config.info.get("task", "undefined_split")
            config.info["_name"] = f"{DATASET_NAME}_{cs}_{i}"
            config = AtomicConfiguration.from_ase(config)
            csf.write(f"{cs},{config.id}\n")
            yield config


def main():
    t0 = time()
    config_generator = read_wrapper(DATASET_FP)
    logging.info("Creating DataManager")
    dm = DataManager(
        configs=config_generator,
        prop_defs=[energy_pd, atomic_forces_pd],
        prop_map=PROPERTY_MAP,
        dataset_id=DATASET_ID,
        standardize_energy=True,
        read_write_batch_size=10000,
    )
    logging.info(f"Time to prep: {time() - t0}")
    t1 = time()
    dm.create_dataset(
        loader=loader,
        name=DATASET_NAME,
        description=DESCRIPTION,
        doi=DOI,
        publication_year=PUBLICATION_YEAR,
        authors=AUTHORS,
        data_license=LICENSE,
        publication_link=PUBLICATION,
        data_link=DATA_LINK,
        equilibrium=False,
    )
    t2 = time()
    logging.info(f"Time to load: {t2 - t1}")
    logging.info(f"Total time: {time() - t0}")
    logging.info("complete")


if __name__ == "__main__":
    main()
